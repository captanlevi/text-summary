{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_summary.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQfQj9Z53FtV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1d5328be-ff69-4685-a043-4cbcbdbd1e01"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import gensim\n",
        "nltk.download('stopwords')\n",
        "google_path = \"drive/My Drive/News/GoogleNews-vectors-negative300.bin\"\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "id803dQIgNrl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "e3c8c716-0d04-4602-9442-9a49c79c441c"
      },
      "source": [
        "# We will be using the google word2vec for getting pre trained embeddings \n",
        "word2vec = gensim.models.KeyedVectors.load_word2vec_format(google_path, binary=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lsq_R0H3gn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The task here is to summarize reviews from amazone\n",
        "data_path =  \"drive/My Drive/test_summary/Reviews.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi-lJkOg4kpx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading data\n",
        "df = pd.read_csv(data_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhgYBY5-5VoD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "outputId": "7b87be16-5615-4963-ad72-0d5b75871f45"
      },
      "source": [
        "# Lets see how it looks like !!!\n",
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>B001E4KFG0</td>\n",
              "      <td>A3SGXH7AUHU8GW</td>\n",
              "      <td>delmartian</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1303862400</td>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>B00813GRG4</td>\n",
              "      <td>A1D87F6ZCVE5NK</td>\n",
              "      <td>dll pa</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1346976000</td>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>B000LQOCH0</td>\n",
              "      <td>ABXLMWJIXXAIN</td>\n",
              "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1219017600</td>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>B000UA0QIQ</td>\n",
              "      <td>A395BORC6FGVXV</td>\n",
              "      <td>Karl</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1307923200</td>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>B006K2ZZ7K</td>\n",
              "      <td>A1UQRSCLF8GW1T</td>\n",
              "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1350777600</td>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id  ...                                               Text\n",
              "0   1  ...  I have bought several of the Vitality canned d...\n",
              "1   2  ...  Product arrived labeled as Jumbo Salted Peanut...\n",
              "2   3  ...  This is a confection that has been around a fe...\n",
              "3   4  ...  If you are looking for the secret ingredient i...\n",
              "4   5  ...  Great taffy at a great price.  There was a wid...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhHh9M0kTIFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We only need the text and Summary\n",
        "df= df[[\"Text\" , \"Summary\"]]\n",
        "df = df.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M3OVxFVFgxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We use the following mapping to convert short hand notations\n",
        "# Credits Kaggle\n",
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "\n",
        "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gWU0A3Z6aU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Borrowing this preprocessing function from kaggle and making some modifications\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower() # lowercase\n",
        "    text = text.split() # convert have'nt -> have not\n",
        "    for i in range(len(text)):\n",
        "        word = text[i]\n",
        "        if word in contraction_mapping:\n",
        "            text[i] = contraction_mapping[word]\n",
        "    newtext = []\n",
        "    for word in text:\n",
        "        if word not in stop_words and \"href\" not in word and \"http\" not in word:\n",
        "            newtext.append(word)\n",
        "    text = \" \".join(newtext)\n",
        "    text = text.replace(\"'s\",'') # convert your's -> your\n",
        "    text = re.sub(r'\\(.*\\)','',text) # remove (words)\n",
        "    text = re.sub(r'[^a-zA-Z ]','',text) # remove punctuations\n",
        "    text = re.sub(' +', ' ', text) # Remove extra spaces\n",
        "    text = re.sub(r'\\\"','',text)\n",
        "    newtxt = []\n",
        "    for w in text.split(\" \"):\n",
        "        if(w not in word2vec):\n",
        "            newtxt = newtxt + word_cleaner(w)\n",
        "        else:\n",
        "            newtxt.append(w)\n",
        "    return \" \".join(newtxt)\n",
        "\n",
        "\n",
        "# The word cleaner function does the following \n",
        "# diningroom --> dining room\n",
        "# This is done so that maximum words are recognized in google word2vec\n",
        "\n",
        "def word_cleaner(w):\n",
        "    L = len(w)\n",
        "    arr = []\n",
        "    i = L\n",
        "    while(i > 0):\n",
        "        if(w[0:i] in word2vec):\n",
        "            arr.append(w[0:i])\n",
        "            break\n",
        "        i = i -1\n",
        "    if(w[i:] in word2vec):\n",
        "        arr.append(w[i:])\n",
        "    return arr\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzTw8prI6dj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Applying preprocessing\n",
        "df[\"Text\"] = df[\"Text\"].apply(lambda x:preprocess(x))\n",
        "df[\"Summary\"] = df[\"Summary\"].apply(lambda x: preprocess(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdkZyADNbn0H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fe80dfa5-bfc8-4c61-f6a8-6413f51dcaf7"
      },
      "source": [
        "# We have a total of 500 thousand Text summary pairs\n",
        "len(df)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "568427"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5u7RieMm0HV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Deleting Text smaller than 10 words \n",
        "max_summary_length = 10\n",
        "def func_text(s):\n",
        "    if(len(s.split(\" \")) <= 10 or len(s.split(\" \")) > 50):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# Deleting summary greater than 10\n",
        "def func_summary(s):\n",
        "    if(len(s.split(\" \")) > max_summary_length):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "# The cut-off were decided after poltting histograms for both text length and summary length\n",
        "df =  df[df[\"Text\"].apply(lambda x:func_text(x))]\n",
        "df =  df[df[\"Summary\"].apply(lambda x:func_summary(x))]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP79Gp0yoopT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arr = []\n",
        "for s in df[\"Text\"]:\n",
        "    arr.append(len(s.split(\" \")))\n",
        "brr = []\n",
        "\n",
        "for s in df[\"Summary\"]:\n",
        "    brr.append(len(s.split(\" \")))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqUVbSKaostS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "01ef9d0a-8889-4afd-b6d5-ddc06c3d82f3"
      },
      "source": [
        "plt.hist(arr)\n",
        "# Ploting the sentence length histogram for Text"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([59343., 65520., 56045., 47462., 41311., 35137., 30338., 26147.,\n",
              "        22870., 19759.]),\n",
              " array([11. , 14.9, 18.8, 22.7, 26.6, 30.5, 34.4, 38.3, 42.2, 46.1, 50. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyhoMfSfownC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "32769ff9-b7ba-49bf-fb85-31fbade0d5ed"
      },
      "source": [
        "plt.hist(brr)\n",
        "# Ploting the sentence length histogram for Summary"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 67271., 132501.,  90208.,  58968.,  28633.,  14369.,   7175.,\n",
              "          2896.,   1356.,    555.]),\n",
              " array([ 1. ,  1.9,  2.8,  3.7,  4.6,  5.5,  6.4,  7.3,  8.2,  9.1, 10. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAS6ElEQVR4nO3db4xd9X3n8fdn7ZKQVOFPGCFqo7Wl\nWKkctN0Qi7iLVEVxCwaimAdJBNot3qwVqwpp06pSanYfICVhRbRVaZASJBS7mCzCQW4qrOLUtYAo\nqlQIQ4gAQygjIGFcCNOYP22jhDr97oP78+YyzA/jueO5g/1+SVf3nO/5nXO+90q+nzl/7nWqCkmS\n5vIfxt2AJGnpMiQkSV2GhCSpy5CQJHUZEpKkruXjbmChnXXWWbVq1apxtyFJbykPPvjgP1XVxOz6\nCRcSq1atYnJyctxtSNJbSpIfzlX3dJMkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJ\nXYaEJKnrhPvG9VvVqm13jWW/z1x/2Vj2K+mtwSMJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5D\nQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1HXUkEiyI8kLSR4dqv2fJD9I\n8nCSv0py+tCya5JMJXkiycVD9Y2tNpVk21B9dZL7W/0bSU5p9be1+am2fNVCvWhJ0pvzZo4kbgE2\nzqrtB86rqv8E/ANwDUCStcAVwPvaOl9NsizJMuArwCXAWuDKNhbgS8ANVfUe4EVgS6tvAV5s9Rva\nOEnSIjpqSFTVd4BDs2p/W1WH2+x9wMo2vQnYVVU/r6qngSnggvaYqqqnqupVYBewKUmADwO72/o7\ngcuHtrWzTe8GNrTxkqRFshDXJP4H8K02vQJ4dmjZdKv16u8GXhoKnCP112yrLX+5jX+dJFuTTCaZ\nnJmZGfkFSZIGRgqJJP8LOAzctjDtzE9V3VxV66pq3cTExDhbkaQTyvL5rpjkvwMfATZUVbXyQeDc\noWErW41O/SfA6UmWt6OF4fFHtjWdZDlwWhsvSVok8zqSSLIR+Bzw0ar66dCiPcAV7c6k1cAa4LvA\nA8CadifTKQwubu9p4XIv8LG2/mbgzqFtbW7THwPuGQojSdIiOOqRRJLbgQ8BZyWZBq5lcDfT24D9\n7VryfVX1e1V1IMkdwGMMTkNdXVW/aNv5DLAPWAbsqKoDbRd/AuxK8kXgIWB7q28Hvp5kisGF8ysW\n4PVKko7BUUOiqq6co7x9jtqR8dcB181R3wvsnaP+FIO7n2bXfwZ8/Gj9SZKOH79xLUnqMiQkSV2G\nhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhI\nkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1HXUkEiyI8kLSR4dqp2ZZH+SJ9vzGa2e\nJDcmmUrycJLzh9bZ3MY/mWTzUP0DSR5p69yYJG+0D0nS4nkzRxK3ABtn1bYBd1fVGuDuNg9wCbCm\nPbYCN8HgAx+4FvggcAFw7dCH/k3Ap4bW23iUfUiSFslRQ6KqvgMcmlXeBOxs0zuBy4fqt9bAfcDp\nSc4BLgb2V9WhqnoR2A9sbMveVVX3VVUBt87a1lz7kCQtkvlekzi7qp5r088DZ7fpFcCzQ+OmW+2N\n6tNz1N9oH5KkRTLyhet2BFAL0Mu895Fka5LJJJMzMzPHsxVJOqnMNyR+3E4V0Z5faPWDwLlD41a2\n2hvVV85Rf6N9vE5V3VxV66pq3cTExDxfkiRptvmGxB7gyB1Km4E7h+pXtbuc1gMvt1NG+4CLkpzR\nLlhfBOxry15Jsr7d1XTVrG3NtQ9J0iJZfrQBSW4HPgSclWSawV1K1wN3JNkC/BD4RBu+F7gUmAJ+\nCnwSoKoOJfkC8EAb9/mqOnIx/NMM7qA6FfhWe/AG+5AkLZKjhkRVXdlZtGGOsQVc3dnODmDHHPVJ\n4Lw56j+Zax+SpMXjN64lSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQk\nqeuoP8uhE9uqbXeNZb/PXH/ZWPYr6dh4JCFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroM\nCUlSlyEhSeoyJCRJXYaEJKnLkJAkdY0UEkn+KMmBJI8muT3J25OsTnJ/kqkk30hyShv7tjY/1Zav\nGtrONa3+RJKLh+obW20qybZRepUkHbt5h0SSFcAfAOuq6jxgGXAF8CXghqp6D/AisKWtsgV4sdVv\naONIsrat9z5gI/DVJMuSLAO+AlwCrAWubGMlSYtk1NNNy4FTkywH3gE8B3wY2N2W7wQub9Ob2jxt\n+YYkafVdVfXzqnoamAIuaI+pqnqqql4FdrWxkqRFMu+QqKqDwJ8CP2IQDi8DDwIvVdXhNmwaWNGm\nVwDPtnUPt/HvHq7PWqdXf50kW5NMJpmcmZmZ70uSJM0yyummMxj8Zb8a+DXgnQxOFy26qrq5qtZV\n1bqJiYlxtCBJJ6RRTjf9NvB0Vc1U1b8B3wQuBE5vp58AVgIH2/RB4FyAtvw04CfD9Vnr9OqSpEUy\nSkj8CFif5B3t2sIG4DHgXuBjbcxm4M42vafN05bfU1XV6le0u59WA2uA7wIPAGva3VKnMLi4vWeE\nfiVJx2je/8d1Vd2fZDfwPeAw8BBwM3AXsCvJF1tte1tlO/D1JFPAIQYf+lTVgSR3MAiYw8DVVfUL\ngCSfAfYxuHNqR1UdmG+/kqRjN++QAKiqa4FrZ5WfYnBn0uyxPwM+3tnOdcB1c9T3AntH6VGSNH9+\n41qS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIk\nJEldhoQkqWuknwo/0azadte4W5CkJcUjCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1DVS\nSCQ5PcnuJD9I8niS30xyZpL9SZ5sz2e0sUlyY5KpJA8nOX9oO5vb+CeTbB6qfyDJI22dG5NklH4l\nScdm1COJLwN/U1W/DvwG8DiwDbi7qtYAd7d5gEuANe2xFbgJIMmZwLXAB4ELgGuPBEsb86mh9TaO\n2K8k6RjMOySSnAb8FrAdoKperaqXgE3AzjZsJ3B5m94E3FoD9wGnJzkHuBjYX1WHqupFYD+wsS17\nV1XdV1UF3Dq0LUnSIhjlSGI1MAP8RZKHknwtyTuBs6vquTbmeeDsNr0CeHZo/elWe6P69Bz110my\nNclkksmZmZkRXpIkadgoIbEcOB+4qareD/wrvzy1BEA7AqgR9vGmVNXNVbWuqtZNTEwc791J0klj\nlB/4mwamq+r+Nr+bQUj8OMk5VfVcO2X0Qlt+EDh3aP2VrXYQ+NCs+rdbfeUc43UCGOePKT5z/WVj\n27f0VjPvI4mqeh54Nsl7W2kD8BiwBzhyh9Jm4M42vQe4qt3ltB54uZ2W2gdclOSMdsH6ImBfW/ZK\nkvXtrqarhrYlSVoEo/5U+O8DtyU5BXgK+CSD4LkjyRbgh8An2ti9wKXAFPDTNpaqOpTkC8ADbdzn\nq+pQm/40cAtwKvCt9pAkLZKRQqKqvg+sm2PRhjnGFnB1Zzs7gB1z1CeB80bpUZI0f37jWpLUZUhI\nkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSp\ny5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqWvkkEiyLMlDSf66za9Ocn+SqSTf\nSHJKq7+tzU+15auGtnFNqz+R5OKh+sZWm0qybdReJUnHZiGOJD4LPD40/yXghqp6D/AisKXVtwAv\ntvoNbRxJ1gJXAO8DNgJfbcGzDPgKcAmwFriyjZUkLZKRQiLJSuAy4GttPsCHgd1tyE7g8ja9qc3T\nlm9o4zcBu6rq51X1NDAFXNAeU1X1VFW9CuxqYyVJi2TUI4k/Bz4H/HubfzfwUlUdbvPTwIo2vQJ4\nFqAtf7mN///1Wev06q+TZGuSySSTMzMzI74kSdIR8w6JJB8BXqiqBxewn3mpqpural1VrZuYmBh3\nO5J0wlg+wroXAh9NcinwduBdwJeB05Msb0cLK4GDbfxB4FxgOsly4DTgJ0P1I4bX6dUlSYtg3kcS\nVXVNVa2sqlUMLjzfU1X/FbgX+Fgbthm4s03vafO05fdUVbX6Fe3up9XAGuC7wAPAmna31CltH3vm\n268k6diNciTR8yfAriRfBB4Ctrf6duDrSaaAQww+9KmqA0nuAB4DDgNXV9UvAJJ8BtgHLAN2VNWB\n49CvJKljQUKiqr4NfLtNP8XgzqTZY34GfLyz/nXAdXPU9wJ7F6JHSdKx8xvXkqQuQ0KS1GVISJK6\nDAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVLX8fgVWGlJW7XtrrHs95nr\nLxvLfqVReCQhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqmndIJDk3yb1J\nHktyIMlnW/3MJPuTPNmez2j1JLkxyVSSh5OcP7StzW38k0k2D9U/kOSRts6NSTLKi5UkHZtRjiQO\nA39cVWuB9cDVSdYC24C7q2oNcHebB7gEWNMeW4GbYBAqwLXAB4ELgGuPBEsb86mh9TaO0K8k6RjN\nOySq6rmq+l6b/mfgcWAFsAnY2YbtBC5v05uAW2vgPuD0JOcAFwP7q+pQVb0I7Ac2tmXvqqr7qqqA\nW4e2JUlaBAtyTSLJKuD9wP3A2VX1XFv0PHB2m14BPDu02nSrvVF9eo66JGmRjBwSSX4V+EvgD6vq\nleFl7QigRt3Hm+hha5LJJJMzMzPHe3eSdNIYKSSS/AqDgLitqr7Zyj9up4pozy+0+kHg3KHVV7ba\nG9VXzlF/naq6uarWVdW6iYmJUV6SJGnIKHc3BdgOPF5Vfza0aA9w5A6lzcCdQ/Wr2l1O64GX22mp\nfcBFSc5oF6wvAva1Za8kWd/2ddXQtiRJi2CU/3ToQuB3gUeSfL/V/idwPXBHki3AD4FPtGV7gUuB\nKeCnwCcBqupQki8AD7Rxn6+qQ23608AtwKnAt9pDkrRI5h0SVfV3QO97CxvmGF/A1Z1t7QB2zFGf\nBM6bb4+SpNH4jWtJUpchIUnqMiQkSV2jXLiWdAxWbbtrbPt+5vrLxrZvvbV5JCFJ6jIkJEldhoQk\nqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnL326STgLj+t0ofzPqrc8jCUlS\nlyEhSeoyJCRJXYaEJKnLkJAkdXl3k6Tjxv+N763PIwlJUteSD4kkG5M8kWQqybZx9yNJJ5Mlfbop\nyTLgK8DvANPAA0n2VNVj4+1M0lLnFwgXxpIOCeACYKqqngJIsgvYBBgSkpakE+06zFIPiRXAs0Pz\n08AHZw9KshXY2mb/JckTi9Db8XQW8E/jbmIJ8f34Jd+L1/L9GJIvjfR+/Me5iks9JN6UqroZuHnc\nfSyUJJNVtW7cfSwVvh+/5HvxWr4fr3U83o+lfuH6IHDu0PzKVpMkLYKlHhIPAGuSrE5yCnAFsGfM\nPUnSSWNJn26qqsNJPgPsA5YBO6rqwJjbWgwnzKmzBeL78Uu+F6/l+/FaC/5+pKoWepuSpBPEUj/d\nJEkaI0NCktRlSCwhSc5Ncm+Sx5IcSPLZcfc0bkmWJXkoyV+Pu5dxS3J6kt1JfpDk8SS/Oe6exiXJ\nH7V/I48muT3J28fd02JKsiPJC0keHaqdmWR/kifb8xkLsS9DYmk5DPxxVa0F1gNXJ1k75p7G7bPA\n4+NuYon4MvA3VfXrwG9wkr4vSVYAfwCsq6rzGNzUcsV4u1p0twAbZ9W2AXdX1Rrg7jY/MkNiCamq\n56rqe236nxl8CKwYb1fjk2QlcBnwtXH3Mm5JTgN+C9gOUFWvVtVL4+1qrJYDpyZZDrwD+Mcx97Oo\nquo7wKFZ5U3Azja9E7h8IfZlSCxRSVYB7wfuH28nY/XnwOeAfx93I0vAamAG+It2+u1rSd457qbG\noaoOAn8K/Ah4Dni5qv52vF0tCWdX1XNt+nng7IXYqCGxBCX5VeAvgT+sqlfG3c84JPkI8EJVPTju\nXpaI5cD5wE1V9X7gX1mg0wlvNe1c+yYGwflrwDuT/LfxdrW01OC7DQvy/QZDYolJ8isMAuK2qvrm\nuPsZowuBjyZ5BtgFfDjJ/x1vS2M1DUxX1ZEjy90MQuNk9NvA01U1U1X/BnwT+C9j7mkp+HGScwDa\n8wsLsVFDYglJEgbnnB+vqj8bdz/jVFXXVNXKqlrF4KLkPVV10v61WFXPA88meW8rbeDk/cn8HwHr\nk7yj/ZvZwEl6EX+WPcDmNr0ZuHMhNmpILC0XAr/L4K/m77fHpeNuSkvG7wO3JXkY+M/A/x5zP2PR\njqZ2A98DHmHwOXZS/TxHktuBvwfem2Q6yRbgeuB3kjzJ4Gjr+gXZlz/LIUnq8UhCktRlSEiSugwJ\nSVKXISFJ6jIkJEldhoQkqcuQkCR1/T96wHMAGvL8swAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xnq171hSoyD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# after all the preprocessing we are left with around 400 thousand training examples "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFx5CjyoTm5f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "d62e8398-cbe2-4b1f-a3a4-1db525c5cd34"
      },
      "source": [
        "# Lets look at a training example\n",
        "print(df[\"Text\"][1154])\n",
        "print(df[\"Summary\"][1154])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ordered three trays found two old stale admit checking outof date exterior package still dried fruit last tray fresher experience soured point i am disappointed\n",
            "mostly dry\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ui06CdxTtg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now lets start making dicts for word to index and viceversa\n",
        "def dict_maker(data):\n",
        "    word2index = {}\n",
        "    index2word = {}\n",
        "    word2index[\"<pad>\"] = 0\n",
        "    word2index[\"<bos>\"] = 1\n",
        "    word2index[\"<eos>\"] = 2\n",
        "    index2word[0] = \"<pad>\"\n",
        "    index2word[1] = \"<bos>\"\n",
        "    index2word[2] = \"<eos>\"\n",
        "    index = 3\n",
        "    text = data[\"Text\"].values\n",
        "    summary = data[\"Summary\"].values\n",
        "\n",
        "    for txt,summ in zip(text,summary):\n",
        "        for word in txt.split(\" \"):\n",
        "            if(word not in word2index):\n",
        "                word2index[word] = index\n",
        "                index2word[index] = word\n",
        "                index += 1\n",
        "        for word in summ.split(\" \"):\n",
        "            if(word not in word2index):\n",
        "                word2index[word] = index\n",
        "                index2word[index] = word\n",
        "                index += 1\n",
        "    return word2index , index2word\n",
        "        \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeUpm_5KUO1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2index , index2word = dict_maker(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUrXy6jeUUa_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "31de16d2-05fd-48f1-f175-8687f5307bab"
      },
      "source": [
        "# lets make and store embedding vectors, so that we no longer have to keep \n",
        "# The google word2vec in RAM memory\n",
        "embedding = torch.empty(len(word2index) + 3 , 300)\n",
        "# This embedding will not be a trainable parameter\n",
        "for i in range(len(word2index)):\n",
        "    w = index2word[i]\n",
        "    if(w not in word2vec):\n",
        "        embedding[i] = torch.rand(300)\n",
        "        print(w)\n",
        "    else:\n",
        "        embedding[i] = torch.tensor(word2vec[index2word[i]])\n",
        "\n",
        "# As expected the only words not present in word2vec are ....."
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<pad>\n",
            "<bos>\n",
            "<eos>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZZ8Kb9tlppc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Time to say goodbye to google word2vec and free up some RAM\n",
        "del word2vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M697xkXeUpQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now lets convert out data to numbers\n",
        "def add_bos_eos(s):\n",
        "    return \"<bos> \" + s + \" <eos>\"\n",
        "def convert_words_to_numbers(s):\n",
        "    arr = []\n",
        "    for w in s.split(\" \"):\n",
        "        arr.append(word2index[w])\n",
        "    return arr\n",
        "\n",
        "df[\"Summary\"] = df[\"Summary\"].apply(lambda x: add_bos_eos(x))\n",
        "df[\"Text\"] = df[\"Text\"].apply(lambda x: convert_words_to_numbers(x))\n",
        "df[\"Summary\"] = df[\"Summary\"].apply(lambda x: convert_words_to_numbers(x)) \n",
        "max_summary_length += 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOjQifkMUwce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ6rIOLe1QkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = np.random.randint(low = 0 ,high = len(df) , size = (50000))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxLRARy4skYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#total = len(df)\n",
        "# Now I did not have enough time to train the entire data so.... I only train on random 50000 points\n",
        "#train_size = int(.8*total)\n",
        "index_train = np.random.randint(low = 0 ,high = len(df) , size = (50000))\n",
        "train_data = df[index]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brvg1ayGdNzj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = df[:10000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8_OPKvbrzl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now lets make a batch feeder function\n",
        "def get_train_batch(data , batch_size , max_summary_size):\n",
        "    index = np.random.randint(low = 0 , high = len(data) -1 , size = batch_size)\n",
        "    batch = data[index]\n",
        "    X_batch = batch[:,0].tolist()\n",
        "    y_batch = batch[:,1].tolist()\n",
        "    # We have a random sample of size \"batch_size\"\n",
        "\n",
        "    X_batch = list(map(lambda x: torch.tensor(x) , X_batch)) # Converting each batch into tensors\n",
        "    y_batch = list(map(lambda x: torch.tensor(x) , y_batch))\n",
        "\n",
        "    X = pad_sequence(X_batch , batch_first= True) # We now pad the shorter sequences with zeros\n",
        "    y = pad_sequence(y_batch , batch_first= True) \n",
        "    y = torch.cat((y,torch.zeros(batch_size, max_summary_size - y.size(1)).long()) , dim = 1)\n",
        "\n",
        "    \n",
        "    return X.to(dtype=torch.long, device=device),y.to(dtype=torch.long, device=device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kclovBBs9Olm",
        "colab_type": "text"
      },
      "source": [
        "***Onto the model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XloF69kLszN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class summarizer(nn.Module):\n",
        "    def __init__(self , embeddings , LSTM_input_dim ,LSTM_hidden_dim , max_summary_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(len(word2index) , 300)\n",
        "        self.embedding.weight = nn.Parameter(embedding)\n",
        "        # Remember we do not wnat to train the embeddings, so we will set its requires grad flag s False\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        self.max_summary_size = max_summary_size\n",
        "      \n",
        "\n",
        "        self.LSTM_hidden_dim = LSTM_hidden_dim\n",
        "        self.LSTM_input_dim = LSTM_input_dim\n",
        "\n",
        "        self.linear1 = nn.Linear(300 ,self.LSTM_input_dim)\n",
        "        self.Encoder_LSTM = nn.LSTM(input_size= self.LSTM_input_dim , hidden_size=self.LSTM_hidden_dim , batch_first = True)\n",
        "\n",
        "        self.attention_layer = nn.Linear(self.LSTM_hidden_dim  , self.LSTM_hidden_dim)\n",
        "        self.Decoder_LSTM = nn.LSTM(input_size = self.LSTM_hidden_dim , hidden_size= self.LSTM_hidden_dim, batch_first= True)\n",
        "\n",
        "        self.context_to_LSTM = nn.Linear(self.LSTM_hidden_dim , self.LSTM_hidden_dim - 300)\n",
        "\n",
        "        self.Decoder_to_final = nn.Linear(self.LSTM_hidden_dim , len(word2index))\n",
        "\n",
        "    def calculate_attention(self, prev_out , encoder_out):\n",
        "        # Here lets calculate the attention energies\n",
        "        seq_len = encoder_out.size(1)\n",
        "        X = self.attention_layer(encoder_out)\n",
        "        X = X.transpose(1,2)\n",
        "        att_energy = torch.bmm(prev_out , X)\n",
        "        att_energy =  F.softmax(att_energy , dim = 2)\n",
        "        return att_energy   # Now it has the shape (batch_size , 1,seq_len)\n",
        "\n",
        "    def decoder_teacher(self ,encoder_out , target_embedding ):\n",
        "        # We decode by forcing the model the correct sequence\n",
        "        # This helps in convergence,but is clearly not how the model works !!!!\n",
        "        batch_size , seq_len , _ = encoder_out.size()\n",
        "        # Initially the previous decoder output is all zeros\n",
        "        prev_out = torch.zeros(batch_size , 1 , self.LSTM_hidden_dim).to(device).double()\n",
        "\n",
        "        # Lets make a tensor to store the final result\n",
        "        final_out = torch.zeros( batch_size, self.max_summary_size, self.LSTM_hidden_dim).to(device).double()\n",
        "\n",
        "        # Initilizing the decoder state vector (with zeros)\n",
        "        decoder_state = (torch.zeros(1, batch_size ,self.LSTM_hidden_dim).to(device).double(),torch.zeros(1, batch_size ,self.LSTM_hidden_dim).to(device).double())\n",
        "\n",
        "\n",
        "        # Next the loop fills the final tensor in sequence order\n",
        "       \n",
        "        for i in range(self.max_summary_size):\n",
        "            # Calculating attention energies\n",
        "            attention_energy = self.calculate_attention(prev_out , encoder_out)\n",
        "            \n",
        "            context = torch.bmm(attention_energy , encoder_out)\n",
        "            context = self.context_to_LSTM(context)\n",
        "            decoder_LSTM_inp = torch.cat((target_embedding[: , i:i+1 , :] , context ), dim = 2)\n",
        "            decoder_out , decoder_state = self.Decoder_LSTM(decoder_LSTM_inp , decoder_state)\n",
        "            final_out[: , i:i+1 , :] = decoder_out\n",
        "            prev_out = decoder_out\n",
        "        return final_out\n",
        "    \n",
        "    def decoder_non_teacher(self , encoder_out , word_embedding):\n",
        "        # here the model takes as input its own generated words\n",
        "        # This is exactely inference is made with this model\n",
        "        batch_size , seq_len , _ = encoder_out.size()\n",
        "        prev_out = torch.zeros(batch_size , 1 , self.LSTM_hidden_dim).to(device).double()\n",
        "        final_out = torch.zeros(batch_size , self.max_summary_size, self.LSTM_hidden_dim).to(device).double()\n",
        "        # Initilizing the decoder state vector (with zeros)\n",
        "        decoder_state = (torch.zeros(1, batch_size ,self.LSTM_hidden_dim).to(device).double(),torch.zeros(1, batch_size ,self.LSTM_hidden_dim).to(device).double())       \n",
        "       \n",
        "       \n",
        "        for i in range(self.max_summary_size):\n",
        "            attention_energy = self.calculate_attention(prev_out , encoder_out)\n",
        "            context = torch.bmm(attention_energy , encoder_out)\n",
        "            context = self.context_to_LSTM(context)\n",
        "            decoder_LSTM_inp = torch.cat((word_embedding , context ), dim = 2)\n",
        "            decoder_out , decoder_state = self.Decoder_LSTM(decoder_LSTM_inp , decoder_state)\n",
        "            final_out[: , i:i+1 , :] = decoder_out\n",
        "            prev_out = decoder_out\n",
        "            with torch.no_grad():\n",
        "                # Now using the model's prediction to feed its next step\n",
        "                decoder_out  = self.Decoder_to_final(decoder_out)\n",
        "                word_index = torch.argmax(decoder_out , dim = 2)                  \n",
        "                word_embedding = self.embedding(word_index)\n",
        "        return final_out\n",
        "\n",
        "    def forward(self , Text , Summary , teacher = True):\n",
        "        text_embedding = self.embedding(Text)\n",
        "        summary_embedding = self.embedding(Summary)\n",
        "\n",
        "        X = self.linear1(text_embedding)\n",
        "\n",
        "        #Encoding\n",
        "        encoder_out  , _ = self.Encoder_LSTM(X)\n",
        "\n",
        "        if(teacher):\n",
        "            final_out = self.decoder_teacher(encoder_out , summary_embedding)\n",
        "        else:\n",
        "            final_out = self.decoder_non_teacher(encoder_out , summary_embedding[:,0,:].unsqueeze(1))\n",
        "        distribution = self.Decoder_to_final(final_out)\n",
        "        return distribution\n",
        "\n",
        "    def Infer(self ,X):\n",
        "        # Here we use the non teacher decoder to get feed the model its own output\n",
        "        with torch.no_grad():\n",
        "            X = self.embedding(X)\n",
        "            X = self.linear1(X)\n",
        "            encoder_out ,_ = self.Encoder_LSTM(X)\n",
        "            embedding_index = torch.ones(X.size(0) ,1).to(device).long()\n",
        "            final_out = self.decoder_non_teacher(encoder_out, self.embedding(embedding_index))\n",
        "            final_out = self.Decoder_to_final(final_out)\n",
        "            final_out = torch.argmax(final_out , dim = -1)\n",
        "        return final_out\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br50wQfItHBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = summarizer(embeddings = embedding ,LSTM_input_dim=500 , LSTM_hidden_dim= 1000 , max_summary_size= max_summary_length ).double().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gEa_EU0L0Aw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = .0001\n",
        "# Removing the embedding from trainable parameters\n",
        "params_model = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
        "optimizer = torch.optim.Adam(params_model, learning_rate)\n",
        "# We use cross entropy loss to train the model\n",
        "crit = torch.nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAcP8gGNtP32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now lets train this beast !!!\n",
        "def loss_func(model_out , labels):\n",
        "    model_out = model_out.view(-1 , model_out.size(-1))\n",
        "    labels = labels.view(-1)\n",
        "    return crit(model_out , labels).mean()\n",
        "\n",
        "def train(model ,epochs, batch_size):\n",
        "    inner_loop_iters = len(train_data)//batch_size\n",
        "\n",
        "    for i in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for _ in range(inner_loop_iters):\n",
        "            X,y = get_train_batch(train_data,batch_size, max_summary_length)\n",
        "\n",
        "            # Here we toss a coin !!!!\n",
        "            # Heads we train with the teacher method\n",
        "            # Tails we trian wth the non teacher method\n",
        "            toss = random.random()\n",
        "            if(toss <= .5):\n",
        "                output = model.forward(X,y, True)\n",
        "            else:\n",
        "                output = model.forward(X,y,False)\n",
        "            loss = loss_func(output , y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            epoch_loss += loss.item()\n",
        "        print(epoch_loss/inner_loop_iters)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh6U_tIIKAVD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "b5b2165c-023f-4204-b9ff-3087fe42b20a"
      },
      "source": [
        "train(model ,20 , batch_size= 50)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8938231541926253\n",
            "0.8814939409819766\n",
            "0.8426032427838239\n",
            "0.8435771515386153\n",
            "0.8078112826369664\n",
            "0.803902057640869\n",
            "0.7947947889309731\n",
            "0.7789755336877264\n",
            "0.7244385859911185\n",
            "0.7285105175049819\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vapz-3eXQXWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now lets put this model to test \n",
        "def decode_sentence(array):\n",
        "    arr = []\n",
        "    for i in array:\n",
        "        arr.append(index2word[i])\n",
        "    return arr\n",
        "\n",
        "def remove_tags(arr):\n",
        "    arr = arr[1:]\n",
        "    new_arr = []\n",
        "    for w in arr:\n",
        "        if(w == \"<eos>\"):\n",
        "            break\n",
        "        new_arr.append(w)\n",
        "    return new_arr\n",
        "\n",
        "def summarize(model, text):\n",
        "    inp = torch.tensor(text).to(device).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        summary = model.Infer(inp)\n",
        "    summary = summary[0].to(\"cpu\").numpy()\n",
        "    text = decode_sentence(text)\n",
        "    summ = decode_sentence(summary)\n",
        "    text = remove_tags(text)\n",
        "    summ = remove_tags(summ)\n",
        "\n",
        "   \n",
        "    print(\"TEXT-----> \" , \" \".join(text))\n",
        "    print(\"SUMMARY--> \",\" \".join(summ ))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Srh-yYvhYG2W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "82eea6ea-bb9a-471e-dcbd-b71d207fdf36"
      },
      "source": [
        "summarize(model ,test_data[8545][0])"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TEXT----->  daughter turning wanted bubble gum theme course daughter remembered growing well tiny size chiclet s looked everywhere found amazon thrilled purchased several packs introduce year old bring back wonderful memories chiclet s gum still wonderful product always was\n",
            "SUMMARY-->  great gift\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_DFnB1XdYP3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "430b5eb2-7994-49e6-90a8-437f2ba2977d"
      },
      "source": [
        "summarize(model ,test_data[8587][0])"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TEXT----->  fit perfectly real space saver right size holds coffee choices\n",
            "SUMMARY-->  great\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv0vMyvmeF06",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a061bc96-c136-45ed-a342-044304c95063"
      },
      "source": [
        "summarize(model ,test_data[7895][0])"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TEXT----->  sweet treat nutritional value fantastic thing find local store lower cost\n",
            "SUMMARY-->  tasty\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPUfd3azeTnr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "ce16f564-37be-465b-9c07-ad40b63d7ee5"
      },
      "source": [
        "summarize(model ,train_data[8545][0])"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TEXT----->  recommend new horn hard art coffee we liberty roast we delighted discover lasting flavor also makes flavorful cappuccino espresso pleased enjoying traditional american coffee brand looking forward future coffee roasts hh coffee\n",
            "SUMMARY-->  great coffee\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2S7x74Yc7qp",
        "colab_type": "text"
      },
      "source": [
        "So yeah it works :) !!!"
      ]
    }
  ]
}